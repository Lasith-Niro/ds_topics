{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (Non-parametric Model)\n",
    "\n",
    "1) kNN Algorithm\n",
    "<img src=knn_classification.png text=\"kNN Classification\" width=50% />\n",
    "* kNN Classification steps:\n",
    "    1. For a data point in training set, calculate distance from data point to new value\n",
    "    2. Order distances in increasing order and take the first $k$\n",
    "    3. Take the label with the most votes\n",
    "* **kNN Regression** - assign mean value of k nearest neighbors\n",
    "    * neighbors averaged (or some combo function) to give continuous value\n",
    "* **kNN Imputation** - replace missing data/values with kNN\n",
    "* **kNN Anomaly Detection** - outlier has large distances to nearest neighbors\n",
    "    * determining if the nearest neighbor is super far away\n",
    "    * is the new data point an outlier?\n",
    "* Determining K:\n",
    "    * use k-fold cross-validation technique to determine the best $k$ for the model\n",
    "<img src=kfold_cv_knn.png text=\"k-Fold Cross-Validation For kNN\" width=50% />\n",
    "* Feature Scaling:\n",
    "<img src=knn_feature_scaling.png text=\"kNN Feature Scaling\" width=90% /> \n",
    "    * Make sure to scale the features\n",
    "    * Similar to Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Distance Metrics:\n",
    "1. **Euclidean distance** - straight-line distance between two points\n",
    "<img src=http://rosalind.info/media/Euclidean_distance.png text=\"Euclidean Distance\" width=50% />\n",
    "    * equation: $dist(a,b) = \\Vert a-b \\Vert = \\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+\\cdots+(a_n-b_n)^2} = \\sqrt{\\sum_{i=1}^n (a_i-b_i)^2}$\n",
    "2. **Cosine similarity** - measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them\n",
    "<img src=https://engineering.aweber.com/wp-content/uploads/2013/02/4AUbj.png text=\"Cosine Similarity\" width=60% />\n",
    "    * equation: $dist(a,b) = \\frac{a \\cdot b}{\\Vert a \\Vert \\Vert b \\Vert} = \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\sqrt{\\sum_{i=1}^n b_i^2}}$\n",
    "3. **Manhattan** - sum of the lengths of the projections of the line segments between the points onto the coordinate axes\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/1200px-Manhattan_distance.svg.png text=\"Manhattan Distance\" width=40% />    \n",
    "    * equation: $dist(a,b) = \\Vert a-b \\Vert_1 = \\sum_{i=1}^n |a_i-b_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Curse of Dimensionality / Pros and Cons\n",
    "* **Curse of Dimensionality** - as the dimensionality increases, performance of kNN commonly decreases\n",
    "    * Nearest neighbors are no longer nearby neighbor\n",
    "    * Adding useful features (truly associated with the response) is generally helpful, but noise features increase dimensionality without the upside\n",
    "    * Intuition: consider a particular red hypercube inside a unit-hypercube space (black)\n",
    "        * how long would we need to make one side of the red hypercube in order to capture 10% of the volume of the space (black)?\n",
    "<img src=curse_of_dim_knn.png text=\"Curse of Dimensionality Intuition\" width=100% /> \n",
    "* kNN Strength and Weaknesses:\n",
    "    * (+) really easy to train, store and update model (saves all data)\n",
    "    * (+) easily works with any number of classes\n",
    "    * (+) easy to add new training data points\n",
    "    * (+) can learn a very complex function\n",
    "    * (+) no demands on relationships between variables (e.g. linearity assumption)\n",
    "    * (+) only a few hyperparameters to tune (e.g. k, distance metric, etc.)\n",
    "    * (-) really slow to predict (especially if you have a lot of features)\n",
    "    * (-) I.O. bound - the cost is reading through data, not calculation\n",
    "    * (-) noise can affect results\n",
    "        * particularly irrelevant dimensions\n",
    "        * even though it is okay to have correlated variables, it is best to remove or else the dimensionality price will be high\n",
    "    * (-) feature interpretation can be tricky\n",
    "        * categorical: how to calculate distance?\n",
    "        * feature scaling makes variables less interpretable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
