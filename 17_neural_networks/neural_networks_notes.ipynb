{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks\n",
    "\n",
    "* Objectives:\n",
    "    * Know the best use cases for neural networks\n",
    "    * Know the benefits and drawbacks of using a neural network\n",
    "    * Build a simple neural network for binary classification\n",
    "    * Train a neural network using backpropagation\n",
    "    * Understand how neural networks can be used for regression and multi-class classification by using different loss functions and output activations\n",
    "    * Explain the properties (pros/cons) of different activation functions\n",
    "    * Explain some methods to avoid overfitting\n",
    "    * Learn about some more complicated versions of neural networks\n",
    "    * Use Keras to build neural networks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Neural Network Basics\n",
    "* Background - neural networks were introduced in the 1950's as a model which mimics the brain\n",
    "    * biological neurons \"fire\" at a certain voltage threshold\n",
    "    * an artifical neuron will be modeled by an activation function like **sign**, **sigmoid function**, or **tanh**\n",
    "    * otherwise, it is bad analogy since we shouldn't be thinking of neural networks as models for the brain\n",
    "* Why use Neural Networks?\n",
    "    * (+) works well with high dimensional data (images, text, and audio)\n",
    "    * (+) can model *arbitrarily* complicated decision functions (complex decision boundaries)\n",
    "    * (-) not very interpretable\n",
    "    * (-) slow to train (very computationally expensive)\n",
    "    * (-) easy to overfit (complex activation that lowers bias, but has high variance)\n",
    "    * (-) difficult to tune (many parameters/choices when building the neural network architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Building Neural Networks Basics\n",
    "\n",
    "* **Activation functions**\n",
    "    * Where have we seen an activation function before? (sigmoid function in Logistic Regression!)\n",
    "        * input: $x$\n",
    "        * weights: $w$\n",
    "        * sigmoid function: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "        * now, classify $x$ as positive if $\\sigma(w^Tx)>0.5$\n",
    "        * think of $\\sigma$ as an **activation function** which **activates** if the input is larger than 0\n",
    "* Creating two layer neural network with sigmoid activation function\n",
    "![sigmoid_two_layer](sigmoid_two_layer.png)\n",
    "    * Architecture:\n",
    "        * **Input layer**: the nodes which hold the inputs: $1,x_1,x_2,\\dots,x_n$\n",
    "        * **Output layer**: the single node that holds the output value\n",
    "        * **Weights**: the weights: $w_0,w_1,\\dots,w_n$ transition between the two layers\n",
    "    * with the current two layer architecture, $h(x|w) = \\sigma(w^Tx)$, only is able to model **linear** decision functions\n",
    "        * the decision boundary is the set of points where $w^Tx=0$, creating a hyperplane\n",
    "* Moving onto Neural Network with Multiple Layers (e.g. 4 layers)\n",
    "![4_layer_nn_tanh_sigma](4_layer_nn_tanh_sigma.png)\n",
    "    * Architecture:\n",
    "        * **Input layer (layer 0)**: contains the input value $x$ and a bias term $1$\n",
    "        * **Two hidden layer (layer 1 and 2)**\n",
    "        * **Output layer (layer 3)**: contains the output value (or the probability of positive classification)\n",
    "    * Compute output for: $x=3$\n",
    "        * Layer 1:\n",
    "            * The first non-bias node $\\rightarrow tanh((0.1)(1)+(0.3)(3))=0.76$\n",
    "            * The second non-bias node $\\rightarrow tanh((0.2)(1)+(0.4)(3))=0.89$\n",
    "        * Layer 2:\n",
    "            * The non-bias node $\\rightarrow tanh((0.2)(1)+(1)(0.76)+(-3)(0.89))=-0.94$\n",
    "        * Output:\n",
    "            * The value of the output layer $\\rightarrow \\sigma((1)(1)+(2)(-0.94))=0.29$\n",
    "        * Finally, $h(3)=0.29$ for $x=3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Neural Network Mathematical Notation\n",
    "* Simplified Neural Network Architecture (At First)\n",
    "    * Stick to networks for binary classification (a single output node)\n",
    "    * Output node will use the sigmoid activation function $\\sigma$\n",
    "    * Hidden layers will use tanh activation function\n",
    "    * $\\theta$ will always represent an activation function (e.g. sign, tanh, sigmoid($\\sigma$), rectifier)\n",
    "* Schematic of Artificial Neuron\n",
    "![artifical_neuron_schematic](artifical_neuron_schematic.png)\n",
    "    * Layers are given by indices: $0,1,2,\\dots,L$\n",
    "        * Input layer: $0$\n",
    "        * Output layer: $L$\n",
    "    * For each layer $l$:\n",
    "        * $s^{(l)} \\rightarrow d^{(l)}$-dimensional input vector\n",
    "        * $x^{(l)} \\rightarrow (d^{(l)}+1)$-dimensional output vector\n",
    "        * $W^{(l)} \\rightarrow (d^{(l-1)}+1)\\times d^{(l)}$ matrix of input weights\n",
    "            * $W_{ij}^{(l)} \\rightarrow$ the weight of the edge from the $i$-th node $l-1$ to the $j$-th node in $l$\n",
    "* Convert Previous 4 Layer Network To Mathematical Notation\n",
    "![4_layer_nn_tanh_sigma](4_layer_nn_tanh_sigma.png)\n",
    "    * Steps For **Forward Propagation**:\n",
    "        1. Multiple Initial Input $x^{(0)}$ by $W^{(1)}$ between layers to yield $s^{(1)}$ through matrix multiplication\n",
    "        2. Apply activation function (e.g. tanh or sigmoid) on $s^{(1)}$ to yield $x^{(1)}$\n",
    "        3. Take the next weights $W^{(2)}$ and apply to previous output of Layer $x^{(1)}$ and repeat cycle until reaching the final output layer $x^{(o)}$\n",
    "    * **Layer 0 $\\rightarrow$ 1:**\n",
    "        * $x^{(0)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            3 \n",
    "            \\end{array}\\right]$\n",
    "        * $W^{(1)}=\\left[\\begin{array}{cc}\n",
    "            0.1 & 0.2 \\\\ \n",
    "            0.3 & 0.4 \n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(1)}$ is the result of applying the weights on the edges between layer 0 and 1: (Applying Weights)\n",
    "            * $\\left[\\begin{array}{cc}\n",
    "                (0.1)(1)+(0.3)(3) \\\\ \n",
    "                (0.2)(1)+(0.4)(3)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                1.4\n",
    "                \\end{array}\\right]$ \n",
    "        * $x^{(1)}$ is the output of layer 1 after applying tanh and adding a bias node: (Output of Layer 1 After Bias/Tanh Function)\n",
    "            * $\\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                tanh(1) \\\\\n",
    "                tanh(1.4)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                0.76 \\\\\n",
    "                0.89\n",
    "                \\end{array}\\right]$ \n",
    "    * **Layer 1 $\\rightarrow$ 2:**\n",
    "        * $W^{(2)}=\\left[\\begin{array}{cc}\n",
    "            0.2 \\\\ \n",
    "            1 \\\\\n",
    "            -3\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(2)}=\\left[\\begin{array}{cc}\n",
    "            (0.2)(1)+(1)(0.76)+(-3)(0.89)\n",
    "            \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                -1.71\n",
    "                \\end{array}\\right]$\n",
    "        * $x^{(2)}=\\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                tanh(-1.71)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                1 \\\\ \n",
    "                -0.94\n",
    "                \\end{array}\\right]$ \n",
    "    * **Layer 2 $\\rightarrow$ 3:**\n",
    "        * $W^{(3)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(3)}=\\left[\\begin{array}{cc}\n",
    "            (1)(1)+(2)(-0.94)\n",
    "            \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                -0.88\n",
    "                \\end{array}\\right]$\n",
    "        * $x^{(3)}=\\left[\\begin{array}{cc}\n",
    "                \\sigma(-0.88)\n",
    "                \\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "                0.29\n",
    "                \\end{array}\\right]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Forward Propagation and Backpropagation\n",
    "* **Forward Propagation** - computing the output of a neural network with fixed weights (based on studying the above examples)\n",
    "    * $x^{(l)}=\\left[\\begin{array}{cc}\n",
    "        1 \\\\ \n",
    "        \\theta(s^{(l)})\n",
    "        \\end{array}\\right]$ (Inputs that have applied activation functions)\n",
    "    * $s^{(1)}=(W^{(l)})^Tx^{(l-1)}$ (Weights that are applied to the Inputs)\n",
    "    * Propagation of computations: $x^{(0)}\\xrightarrow{W^{(1)}}s^{(1)}\\xrightarrow{\\theta}x^{(1)}\\xrightarrow{W^{(2)}}s^{(2)}\\cdots\\rightarrow s^{(L)}\\xrightarrow{\\theta}x^{(L)}=h(x^{(0)})$\n",
    "    * In terms of the number of nodes $V$ and weights $E$, what is the algorithmic complexity of forward propagation (in Big-O notation)?\n",
    "* **Backpropagation** - finds the error based on some function via gradient descent and modifies weights (thereby improving the model) based on the predictions made by the Neural Network in Forward Propagation\n",
    "    * Training data = $\\{(x_i,y_i)\\}$\n",
    "    * Need to minimize some error function $E$ on our training set over the weights: $w = (W^{(1)},\\dots,W^{(L)})$\n",
    "        * Example error function, MSE: $E(w)=\\frac{1}{N}\\sum_{i=1}(h(x_i|w)-y_i)^2$\n",
    "    * This function can be *extremely* complicated to write algebraically and has no closed form solution for minima\n",
    "    * Use gradient descnet algorithm to train neural network (called Backpropagation)\n",
    "        * Update step in gradient descent: $w(t+1)=w(t)-\\eta\\triangledown E(w(t))$ \n",
    "    * Our total error is a sum of the errors, $e_n$, on each input:\n",
    "        * $E(w)=\\frac{1}{N}\\sum_{i=1}^n e_i$ where $e_i=(h(x_i|w)-y_i)^2$\n",
    "        * take derivative with respect to weights: $\\frac{\\partial E}{\\partial W^{(l)}}=\\frac{1}{N}\\sum{\\frac{\\partial e_n}{\\partial W^{(l)}}}$ (need to review how to take derivatives)\n",
    "        * can consider one data point at a time and add the results to get the total gradient\n",
    "    * Backpropagation uses the **chain rule** to compute the partial derivatives of layer $l$ in terms of layer $l+1$\n",
    "        * the **sensitivity vector** of layer $l$: $\\delta^{(l)}=\\frac{\\partial e}{\\partial s^{(l)}}$\n",
    "        * then, we can compute: $\\frac{\\partial e}{\\partial W^{(l)}} = x^{(l-1)}(\\delta^{(l)})^T$\n",
    "        * for $j$ in $1,\\dots,d^{(l)}$: $\\delta_j^{(l)}=\\theta'(s^{(l)})_j \\times [W^{(l+1)}\\delta^{(l+1)}]_j$\n",
    "        * can compute $\\delta^{(l)}$ from $\\delta^{(l+1)}$\n",
    "        * must still compute $\\delta^{(L)}$ to seed the process\n",
    "            * depends on the error function and the output activation function\n",
    "            * in this case: $\\delta^{(L)}=2(h(x_i|w)-y_i)h(x_i|w)(1-h(x_i|w))$\n",
    "        * $W^{(l)}=W^{(l)}-\\eta\\frac{\\partial E}{\\partial W^{(l)}}$\n",
    "* Complete Computation for Forward propagation and Backpropagation example (using the 4 layer NN from above):\n",
    "    * Forward propagation:\n",
    "        * Data is $x=2,y=1$\n",
    "        * $x^{(0)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2 \n",
    "            \\end{array}\\right]$; \n",
    "            $s^{(1)}=\\left[\\begin{array}{cc}\n",
    "            0.1 & 0.3 \\\\ \n",
    "            0.2 & 0.4\n",
    "            \\end{array}\\right]\n",
    "            \\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            2\n",
    "            \\end{array}\\right]=\n",
    "            \\left[\\begin{array}{cc}\n",
    "            0.7 \\\\ \n",
    "            1\n",
    "            \\end{array}\\right]$;\n",
    "            $x^{(1)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            0.6 \\\\\n",
    "            0.76\n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(2)}=\\left[\\begin{array}{cc}\n",
    "            -1.48 \n",
    "            \\end{array}\\right]$; \n",
    "            $x^{(2)}=\\left[\\begin{array}{cc}\n",
    "            1 \\\\ \n",
    "            -0.90 \n",
    "            \\end{array}\\right]$\n",
    "        * $s^{(3)}=\\left[\\begin{array}{cc}\n",
    "            -0.8 \n",
    "            \\end{array}\\right]$; \n",
    "            $x^{(3)}=\\left[\\begin{array}{cc}\n",
    "            0.31 \n",
    "            \\end{array}\\right]$\n",
    "    * Backpropagation:\n",
    "        * $\\delta^{(3)}=2(0.31-1)(0.31)(1-0.31)=-0.30$\n",
    "        * $\\delta^{(2)}=(1-0.9^2)(2)(-0.30)=-0.114$\n",
    "        * $\\delta^{(1)}=\\left[\\begin{array}{cc}\n",
    "            -0.072 \\\\\n",
    "            0.144\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(1)}}=x^{(0)}(\\delta^{(1)})^T=\\left[\\begin{array}{cc}\n",
    "            -0.072 & 0.144 \\\\\n",
    "            -0.144 & 0.288\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(2)}}=x^{(1)}(\\delta^{(2)})^T=\\left[\\begin{array}{cc}\n",
    "            -0.69 \\\\\n",
    "            -0.42 \\\\\n",
    "            -0.53\n",
    "            \\end{array}\\right]$\n",
    "        * $\\frac{\\partial e}{\\partial W^{(3)}}=x^{(2)}(\\delta^{(3)})^T=\\left[\\begin{array}{cc}\n",
    "            -1.85 \\\\\n",
    "            1.67\n",
    "            \\end{array}\\right]$\n",
    "* Another example with Forward and Backpropagation:\n",
    "![example_nn](https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5) Stochastic Gradient Descent in Backpropagation\n",
    "* Backpropagation finds the gradient at each observation, adds them up to find the total gradient:\n",
    "    * $\\triangledown E(w)=\\frac{1}{N}\\sum_{i=1}\\triangledown e_i(w)$\n",
    "    * $w(t+1)=w(t)-\\eta\\triangledown E(w(t))$\n",
    "* Instead, **update weights** at **each** observation (or after a small batch of observations):\n",
    "    * $w(t+1)=w(t)-\\eta\\triangledown e_i(w(t))$\n",
    "    ![sgd_backprop](sgd_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Neural Network Parameter Tuning\n",
    "* **Learning rate**\n",
    "* **Number of hidden layers**\n",
    "* **Number of neurons on hidden layers**\n",
    "* **Initialization of weights**\n",
    "    * What happens if you set weights to 0 or weights very large?\n",
    "    * don't set weights to 0, instead set sample weights as normal centered around 0\n",
    "    * rule of thumb:\n",
    "        * sample weights from $N(0,\\sigma^2_w)$\n",
    "        * $\\sigma^2_w max_i \\Vert x_i\\Vert^2 << 1$\n",
    "* **Scaling** - normalize data before fitting data to neural network model (depending on the activation function)\n",
    "* **Epoch / Batches** - a single sweep through all of the data\n",
    "    * example: if you have 100,000 observations, and a batch size of 100. Then, each epoch will consist of 1,000 gradient descent update steps \n",
    "* **Termination** - error function is generally an **extremely** non-convex function function\n",
    "    * Lots of local minimia and flat spots\n",
    "    * Often best to terminate after a set number of iterations\n",
    "    * Also, can terminate when the gradient is small and the total error is small\n",
    "* **Momentum** - helps push backpropagation out of local minima (step size)\n",
    "    * adds a fraction of the previous gradient in the new update step: $w(t+1)=w(t)-\\eta\\triangledown E(w(t))+m(w(t)-w(t-1))$\n",
    "    * $m=0.9$ is standard\n",
    "    * Too high of $m$ risks overshooting minimum\n",
    "    * Too lows of $m$ risks getting stuck in local minima\n",
    "* **Activation functions**\n",
    "    1. **Sigmoid**\n",
    "        * equation: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "    2. **Tanh** - a hyperbolic tangent function\n",
    "    ![tanh_func](tanh_func.png)\n",
    "        * equation: $tanh(z)=2\\sigma(2z)-1$\n",
    "        * same shape as the sigmoid function\n",
    "        * output values centered around 0\n",
    "        * smooth or differentiable (unlike sign)\n",
    "            * $tanh(z)=2\\sigma(2z)-1 \\rightarrow tanh'(z)=1-tanh^2(z)$\n",
    "        * trains faster than sigmoid in most cases\n",
    "        * (-) requires normalization of the data\n",
    "    3. **Softmax**\n",
    "        * equation: $\\frac{e^{s_j^{(L)}}}{\\sum_{i=1}^K e^{s_i^{(L)}}}$\n",
    "        * useful activation function for output layer of a multi-class classification neural net\n",
    "    4. **Rectifer / Hard Max / Rectified Linear Unit (ReLU)** - a function that maps $\\{0,x\\}$\n",
    "        * gradient does not vanish as $x$ gets large\n",
    "        * 0 if $x<0$, which introduces sparsity into the network\n",
    "        * has faster training\n",
    "        * (+) doesn't require normalization of data\n",
    "* **Overfitting in NN**\n",
    "    * **Regularization** - add a $l2$-regularization term: $\\lambda\\sum_{l,i,j}(w_{ij}^{(l)})^2$ to the error function\n",
    "    * **Dropout** - randomly remove nodes from the network each time you run backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "7) Regression and Multi-Class Classification\n",
    "* Neural Network can also be used for regression and for data with multiple classes\n",
    "    * For regression, replace the output transformation with $\\theta \\rightarrow id$ and use MSE\n",
    "    * For multiple classes:\n",
    "        * The output layer will have multiple ndoes, one for each class $1,2,\\dots,K$\n",
    "        * The output activation function on the $j$-th component of the output layer is the **softmax function**: $\\frac{e^{s_j^{(L)}}}{\\sum_{i=1}^K e^{s_i^{(L)}}}$\n",
    "        * Minimize **cross-entropy**: $\\sum_{i=1}-y_i log h(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Types of Network Neural Architecture\n",
    "1. **Fully Connected Neural Networks** - architecture where each layer is *fully connected* to next and has no missing edges between nodes\n",
    "![full_nn](full_nn.png)\n",
    "    * **Deep Learning Neural Network** - a buzzword referring to a neural network with more than 3 layers\n",
    "2. **Recurrent Neural Networks (RNN)** - a NN where a hidden layer feeds back into itself\n",
    "    * This allows the NN to exhibit dynamic temporal behavior\n",
    "    * RNN's provide \"internal\" memory for sequence processing\n",
    "    * **Long Short Term Memory (LSTM)** - a special kind of RNN capable of learning **long-term dependencies**\n",
    "    * They are very applicable to handwriting/speech recognition\n",
    "        * e.g. Used by Google Translate application\n",
    "        * e.g. Used to train AI on human negotiations\n",
    "![rnn](rnn.png)\n",
    "3. **Convolutional Neural Networks (CNN)** - architecture isn't fully connected and employs convolution layers\n",
    "![cnn](cnn.png)\n",
    "    * used mainly for image classification (state of the art)\n",
    "    * **Convolution Layers** - each node only \"sees\" a subset of the previous layer's nodes\n",
    "        * applies convolutions (type of filter) to each sub-image to \"look for\" certain patterns or shapes (which are learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3) Deep Learning Achievements\n",
    "1. Text\n",
    "    * Google Translate with RNN\n",
    "    * Facebook Human Negotiation AI with RNN\n",
    "2. Voice\n",
    "    * Google DeepMind autoregressive full-convolution WaveNet (A generative model for raw audio) with PixelRNN/PixelCNN\n",
    "    * Google DeepMind Lip reading from television dataset using LSTM + CNN\n",
    "    * WashU Obama Synchronize Lip with RNN\n",
    "3. Computer Vision\n",
    "    * Google Brain enhances Google Maps with OCR (Optical Character Recognition to recognize street signs and store signs using CNN + LSTM\n",
    "    * Google Deepmind Visual Reasoning on CLEVR dataset with 95.5% accuracy using pre-train LSTM\n",
    "    * Uizard pix2code (GUI interpreted by NN into code) with 77% accuracy\n",
    "    * Google SketchRNN trained on detailed vector representations of drawings using Sequence-to-Sequence Variational Autoencoder (VAE) RNN\n",
    "    * **Generative Adversial Networks (GANs)** - competition of two networks (generator and discriminator)\n",
    "        * e.g. First network creates a picture, and the second one tries to understand whether the picture is real or generated\n",
    "        * Orange Labs France Face Aging with Conditional GANs using IMDB dataset\n",
    "        * Google Improves Professional Photos using GANs with Google Street View dataset\n",
    "        * MichU Synthesization of an image from a text description using GANs\n",
    "        * Berkeley AI Research (BAIR) Image-to-Image Translation with Conditional GANs (e.g. creating a map using a satellite image, or realistic texture of the objects using their sketch)\n",
    "        * Christopher Hesse uses UNet and PatchGAN to make nightmare cat demo\n",
    "        * Authors of Pix2Pix develops CycleGAN for transfer between different domains of images\n",
    "        * Using Adversial Autoencoder (AAE) to find new drugs to fight cancer\n",
    "        * Improvements in Adversial Attacks (tricking NN by injecting noise from recognition) using Fast Gradient Sign Method (FGSM) - important in face recognition/self-driving algorithm from being attacked\n",
    "4. **Reinforcement Learning (RL)** - learn the successful behavior of the agent in an environment that gives a reward through experience (e.g. people learning throughout their lives) - used actively in games (e.g. AlphaGO) robots, and system management (e.g. traffic)\n",
    "    * Google DeepMind Deep Q-network (DQN) plays arcade games better than humans (currently being taught to play complex games like Doom)\n",
    "        * Introduction of additional losses (auxiliary tasks), such as the prediction of a frame change (pixel control) so that the agent better understands the consequences of the actions, significantly speeds up learning\n",
    "    * OpenAI Learning Robots using RL (one-shot learning) by actively studying an agent's training by humans in a virtual environment\n",
    "        * A person shows in VR how to perform a certain task, and one demonstration is enouhg for the algorithm to learn it and then reproduce it in real conditions\n",
    "    * OpenAI/Google DeepMind Learning on Human Preferences using RL\n",
    "        * An agent has a task, and the algorithm provides two possible solutions for the human and indicates which one is better\n",
    "    * Google Deepmind Movement in Complex Environments\n",
    "        * Teaching a robot complex behavior (walk, jump, etc.) using agents (body emulators) to perform complex actions by constructing a complex environment with obstacles and with a simple reward for progress in movement\n",
    "5. Other\n",
    "    * Google Deepmind Cools Data Center (reducing energy costs) based on info on thousands of sensors predicts Power Usage Effectiveness (PUE) using NN ensemble\n",
    "    * Google Brain One Model For All Tasks (currently trained models are poorly transferred from task to task) (tensor2tensor)\n",
    "        * Train a model that performs eight tasks from different domains (text, speech, images)\n",
    "    * Facebook Learn Imagenet in one hour (using Tesla P100 - a cluster of 256 GPUs) using Gloo and Caffe2 for distributed learning\n",
    "6. News\n",
    "    * Self-driving Cars\n",
    "        * Intel MobilEye\n",
    "        * Google Waymo\n",
    "    * Healthcare\n",
    "        * Google Deepmind in Healthcare for medical diagnosis\n",
    "    * Investments\n",
    "        * China invests \\$150 Billion in AI\n",
    "        * Baudi Research employs 1,300 people\n",
    "        * Alibaba runs 100 billion samples with a trillion parameters with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
