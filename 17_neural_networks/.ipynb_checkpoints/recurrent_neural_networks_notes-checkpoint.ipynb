{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks\n",
    "\n",
    "* Objectives:\n",
    "    * Review of what neural networks are (Multilayer Perceptron)\n",
    "    * Simple RNN vs MLP\n",
    "    * Benefits of Intralayer Recurrent Connections\n",
    "    * Example of RNN in text data\n",
    "    * Multilayer RNNs\n",
    "    * Keras Neural Network API For RNN\n",
    "    * LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Recurrent Neural Networks Basics\n",
    "* What distinguishes neural networks is that connections between neurons can form a **directed cycle**. This gives a network the ability to maintain a state based on previous input. So it can model **temporal, sequential** behavior\n",
    "* RNN deep-learning models can process:\n",
    "    * **Text** - understood as sequences of word or sequences of characters\n",
    "    * **Timeseries**\n",
    "    * **Sequence data**\n",
    "* Two fundamental deep-learning algorithms for **sequence processing**:\n",
    "    * Recurrent neural network\n",
    "    * **1D Convnets** - 1-dimensional version of 2D convnets\n",
    "* RNN Use Cases:\n",
    "    * **Pattern recognition**: Handwriting, Captioning Images\n",
    "        * **Document/Timeseries classification** - such as identifying the topic of an article or the author of a book\n",
    "        * **Sentiment Analysis** - such as classifying the sentiment of tweets or movie reviews as positive or negative\n",
    "    * **Sequential data**: Speech Recognition, Stock price prediction, generating text, and news stories\n",
    "        * **Timeseries comparisons** - such as estimating how closely related two documents or two stock tickers are\n",
    "        * **Sequence-to-sequence learning** - such as decoding an English sentence into French\n",
    "        * **Timeseries forecasting** - such as predicting the future weather at a certain location, given recent weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Working with Text Data\n",
    "* We can use text to produce a basic form of natural-language understanding, sufficient for applications including **document classification**, **sentiment analysis**, **author identification**, and even **question-answering (QA)** (in a constrained context)\n",
    "* No deep-learning model **truly** understands text in a **human sense**, rather these models can **map the statistical structure of written language**, which is sufficient to solve many simple textual tasks\n",
    "* Deep learning for natural language processing is **pattern recognition applied to <u>words, sentences, and paragraphs</u>**, in much the same way that computer vision is **pattern recognition applied to <u>pixels</u>**\n",
    "* Like all other neural networks, deep-learning models don't take as input **raw text**: they only take **numeric tensors**\n",
    "* **Vectorizing text** - the process of transforming text into numeric tensors. This can be done in **multiple ways**:\n",
    "    * Segment text into **words**, and transform each word into a vector\n",
    "    * Segment text into **characters**, and transform each character into a vector\n",
    "    * Extract **n-grams of words or characters**, and transform each n-gram into a vector\n",
    "        * **N-grams** - overlapping groups of multiple consecutive words or characters\n",
    "            * Extracting **n-grams** is a form of feature engineering, which deep-learning does away with it, replacing it with **hierarchy feature learning**\n",
    "            * 1-D convnets and recurrent neural networks are capable of learning representations for groups of words and characters **without being explicitly told about the existence of such groups** by looking at continuous word or character sequences\n",
    "        * e.g. \"bag-of-2-grams\" for sentence \"The cat sat on the mat.\" $\\rightarrow$ `{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}`\n",
    "        * e.g. \"bag-of-3-grams\" for sentence \"The cat sat on the mat.\" $\\rightarrow$ `{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}`\n",
    "        * **Bag** - refers to the fact that we're dealing with a **set of tokens** rather than a list or sequence (tokens have no specific order)\n",
    "            * **Bag of words** - the tokenization method of taking words and converting them into n-grams of tokens with no specific order\n",
    "            * Since bag-of-words isn't an **order-preserving** tokenization method:\n",
    "                * The general structure of the sentences is lost\n",
    "                * It tends to be used in **shallow language-processing models** (e.g. logistic regression and random forest) rather than in deep-learning models\n",
    "* **Tokens** - collectively, the **different units** you can **break down text** (words, characters, or n-grams)\n",
    "    * **Tokenization** - the process of breaking text into tokens\n",
    "    ![text_token_vector](text_token_vector.png)\n",
    "    * All text-vectorization processes consist of **applying some tokenization scheme** and then associating **numeric vectors with the generated tokens**\n",
    "    * These vectors, packed into **sequence tensors**, are fed into deep neural networks\n",
    "    * There are multiple ways to **associate a vector with a token**:\n",
    "        * **One-hot encoding** - consists of associating a unique integer index with every word and then turning this integer index `i` into a binary vector of size `N` (the size of the vocabulary). The vector is all zeros except for the `i`th entry, which is 1\n",
    "            * The most common, most basic way to turn a token into a vector\n",
    "            * Keras has built-in utilities for doing one-hot encoding, which take care of important features like stripping special characters from strings, only taking into account the `N` most common words in dataset (a common restriction to avoid dealing with a very large input vector spaces)\n",
    "            * Vectors obtained through **one-hot encoding** are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary) -- **sparse vectors**\n",
    "            * (-) One-hot encoding words generally leads to vectors that are 20,000-dimensional or greater (e.g. capturing a vocabulary of 20,000 tokens)\n",
    "            * **One-hot hashing trick** - **hash words into vectors of fixed size** instead of explicitly assigning an index to each word and keeping a reference of these indices in a dictionary\n",
    "                * Use when the **number of unique tokens** in your vocabulary is **too large** to handle explicitly\n",
    "                * Saves memory and allows online encoding of data\n",
    "                * (-) Susceptible to **hash collisions**, where two different words may end up with the same hash, and thus any ML model looking at these hashes won't be able to tell the difference between these words\n",
    "                * The likelihood of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed\n",
    "        * **Token embedding / Word embeddings** - uses **dense word vectors** which are low-dimensional floating-point vectors\n",
    "            * Typically used exclusively for words\n",
    "        ![onehot_vs_wordemb](onehot_vs_wordemb.png)\n",
    "            * Word embeddings are **learned from data**\n",
    "            * It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when **dealing with very large vocabularies**\n",
    "            * (+) Word embeddings **pack more information** into **far fewer dimensions**\n",
    "            * There are two ways to obtain word embeddings:\n",
    "                * Learn word embeddings **jointly** with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you start with **random word vectors** and then **learn vectors in the same way you learn weights of neural network**\n",
    "                * Load into your model word embeddings that you were **precomputed using a different ML task** than the one you're trying to solve (**Pretrained word embeddings**)\n",
    "            * Method 1: **Learning Word Embeddings with the Embedding Layer**\n",
    "                * Simplest way to associate a dense vector with a word is to choose the vector at **random**. \n",
    "                    * (-) The problem with this approach is that the resulting embedding space has **no structure** (e.g. the word \"accurate\" and \"exact\" may end up with completely different embeddings, even thought they're interchangeable in most sentences)\n",
    "                    * (-) It's difficult for a NN to make sense of such a **noisy, unstructured embedding space**\n",
    "                * The **geometric relationships** between word vectors should reflect the **semantic relationships** between the words\n",
    "                ![word_emb_space](word_emb_space.png)\n",
    "                    * Word embeddings are meant to map **human language** into a **geometric space**\n",
    "                    * In a reasonable embedding space, we should expect **synonyms** to be embedded into **similar word vectors**\n",
    "                    * We should expect the geometric distance (e.g. L2 distance) between any two word vectors to relate to the semantic distance between associated words (words meaning **different things** are embedded at points **far away from each other**, whereas **related words are closer**)\n",
    "                    * May also want the **specific directions** in embedding space to be meaningful\n",
    "                        * e.g. dog to wolf or cat to tiger vector can be interpreted as \"pet to wild animal\" vector\n",
    "                        * e.g. wolf to tiger or dog to cat vector can be interpreted as \"canine to feline\" vector\n",
    "                * There is probably an ideal word-embedding space that perfectly maps human language and can be used for any natural language task, but unfortunately it hasn't been done yet\n",
    "                    * Also, human language isn't a thing since there are many different languages that are **not isomorphic** since language is a **reflection of a specific culture and a specific context**\n",
    "                    * More pragmatically, what makes a good word-embedding space depends heavily on your task\n",
    "                        * e.g. The **perfect word-embedding space** for an English-language **movie review sentiment analysis model** may look **different** from the perfect embedding space for an English-language **legal documentation classification model** because the importance of certain semantic relationships **varies from task to task**\n",
    "                * It is reasonable to **learn** a new embedding space with every new task using the weights of a layer (**`Embedding` layer**)\n",
    "            * Method 2: **Using Pretrained Word Embedding**\n",
    "                * Problem: Very little training data available to  learn appropriate task-specific embedding of the vocabulary\n",
    "                * **Load embedding vectors** from a **precomputed embedding space** that you know is highly structured and exhibits useful properties (capturing generic aspects of language structure like common visual features or semantic features)\n",
    "                * Examples of such word embeddings are generally computed using **word-occurrence statistics** (observations about what words co-occur in sentences or documents) that use a variety of techniques\n",
    "                    * In 2000, the idea of a dense, low-dimensional embedding space for words for unsupervised learning was explored\n",
    "                    * In 2013, at Google, one of the most famous and successful word-embedding schemes: **Word2Vec** algorithm was released. **Word2Vec** dimensions capture specific semantic properties (e.g. gender)\n",
    "                * There are variety of **precomputed word embeddings** that you can use in a Keras `Embedding` layer:\n",
    "                    * **Word2Vec**\n",
    "                    * **Global Vectors for Word Representation (GloVe)** - developed in 2014 by Stanford researchers that is a embedding technique based on **factorizing a matrix of word co-occurrence statistics**. Its developers have made available precomputed embeddings for millions of English tokens obtained from Wikipedia/Common Crawl data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Understanding **Recurrent Neural Networks (RNN)**\n",
    "* A major characteristic of all neural networks so far (such as densely connected networks and convnets) is that they have **no memory**\n",
    "    * With such networks, in order to **process a sequence or temporal series of data points**, you have to **show the entire sequence to the network at once** (turning it into a single data point) -- **Feedforward Networks**\n",
    "        * e.g. An entire movie review is transformed into a single large vector and processed in one go\n",
    "* Biological intelligence **processes information incrementally** while **maintaining an internal model of what's it's processing**, built from **past information** and **constantly updated as new information comes in**\n",
    "    * e.g. Reading a present sentence, processing it word by word while **keeping memories** of what came before giving a **fluid representation of the meaning** conveyed by this sentence\n",
    "* A **recurrent neural network** adopts the same principle in an extremely simplified version: it processes sequences by **iterating through the sequences** elements and **maintaining a <u>state</u>** containing information relative to what it has seen so far\n",
    "![rnn_loop](rnn_loop.png)\n",
    "    * In effect, an RNN is a type of neural network that has an **internal loop**\n",
    "    * The state of the RNN is **reset between processing two different, independent sequences** (e.g two different IMDB reviews), so you still **consider one sequence a single data point** (a single input to the network)\n",
    "    * What changes is that this data point is **no longer processed in a single step**; rather, the network internally **loops over sequence elements**\n",
    "    ![mlp_vs_rnn](mlp_vs_rnn.png)\n",
    "        * The double arrow in RNN indicates a weight in each direction (2 weights)\n",
    "        * How many weights are in each architecture?\n",
    "            * Vanilla MLP weights:\n",
    "                * $W_{h\\rightarrow y} = 4$\n",
    "                * $W_{x\\rightarrow h} = 8$\n",
    "            * Vanilla RNN weights:\n",
    "                * $W_{h\\rightarrow y} = 4$\n",
    "                * $W_{h\\rightarrow h} = 16$\n",
    "                * $W_{x\\rightarrow h} = 8$\n",
    "            * Each input shown to them is processed independently, with no state kept in between inputs\n",
    "* Pseudocode for RNN:\n",
    "    ```python\n",
    "    state_t = 0 # state at t\n",
    "    for input_t in input_sequence: # iterates over sequence elements\n",
    "        output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
    "        state_t = output_t # the previous output becomes the state for the next iteration\n",
    "    ```\n",
    "    * The code above is an example of a forward pass of toy RNN in `numpy`\n",
    "    * This RNN takes as **input a sequence of vectors**, which you'll **encode as a 2D tensor of size**: `(timesteps, input_features)`\n",
    "    * It loops over timesteps, and at each timestep, it considers its current state at `t` and the input at `t` of shape `(input_features,)`, and combines them to obtain the output at `t`\n",
    "    * Then, **set the state** for the **next step** to be this **previous output**\n",
    "    * For the **first timestep**, the previous output **isn't defined**; hence, there is **no current state**\n",
    "    * Thus, initialize first timestep with the state as **an all-zero vector** called **<u>initial state</u>** of the network\n",
    "    * The `output_t` function is the **transformation of the input and state** that will be parameterized by **two matrices, `W` and `U`, and a `bias` vector**\n",
    "* `Numpy` implementation of simple RNN:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    timesteps = 100 # number of timesteps in the input sequence\n",
    "    input_features = 32 # dimensionality of the input feature space\n",
    "    output_features = 64 # dimensionality of the output feature space\n",
    "\n",
    "    inputs = np.random.random((timesteps, input_features)) # input data: random noise for the sake of the example\n",
    "\n",
    "    state_t = np.zeros((output_features, )) # initial state: an all-zero vector\n",
    "\n",
    "    # creates random weight matrices\n",
    "    W = np.random.random((output_features, input_features))\n",
    "    U = np.random.random((output_features, output_features))\n",
    "    b = np.random.random((output_features, ))\n",
    "\n",
    "    successive_outputs = []\n",
    "    for input_t in inputs: # input_t is a vector of shape (input_features,)\n",
    "        output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) # combines the input with the current state (the previous output) to obtain the current output\n",
    "\n",
    "        successive_outputs.append(output_t) # stores this output in a list\n",
    "        state_t = output_t # updates the state of the network for the next timestep\n",
    "    final_output_sequence = np.concatenate(successive_outputs, axis=0) # the final output is a 2D tensor of shape (timesteps, output_features)\n",
    "    ```\n",
    "    * In summary, an RNN is a **`for` loop** that **reuses quantities computed during the previous iteration** of the loop, nothing more\n",
    "* There are many different RNNs that are essentially characterized by their **step function**\n",
    "    * e.g. `output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)`\n",
    "* A simple RNN, unrolled over time:\n",
    "![rnn_unroll](rnn_unroll.png)\n",
    "    * In this example, the final output is a 2D tensor of shape `(timesteps, output_features)`, where each timestep is the output of the loop at time `t`\n",
    "    * Each timestep `t` in the output tensor contains information about timesteps `0` to `t` in the input sequence (**about the entire past**)\n",
    "    * For this reason, in many cases, you **<u>don't</u> need this full sequence of outputs**; you **just need the <u>last output</u>** (`output_t` at the end of the loop) because it **already contains information about the <u>entire sequence</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) A recurrent layer in Keras\n",
    "* **Keras** - a high-level neural networks API, written in Python and capable of running on top of either Tensorflow, CNTK, or Theano\n",
    "    * Will become Tensorflow's default API\n",
    "    * Available Recurrent Layers:\n",
    "        * **Recurrent**\n",
    "        * **SimpleRNN**\n",
    "        * **Long Short-Term Memory (LSTM)** (1997)\n",
    "        * **Gated Recurrent Unit (GRU)** (2014)\n",
    "* **`SimpleRNN`** - the proces implemented in `numpy` above is an actual layer in Keras (`from keras.layes import SingleRNN`)\n",
    "    * One minor difference is that `SimpleRNN` processes **batches of sequences**, not single sequence like in the `numpy` example\n",
    "    * This means it takes inputs of shape `(batch_size, timesteps, input_features)`\n",
    "    * Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: (controlled by `return_sequences`)\n",
    "        * It can return the **full sequence** of successive outputs for each timestep: a 3D tensor of shape `(batch_size, timesteps, input_features)`\n",
    "        * Return only the **last output** for each input sequence: a 2D tesnor shape `(batch_size, output_features)`\n",
    "    * It is sometimes useful to **stack** recurrent layers one after the other in order to increase the representational power of a network\n",
    "        * In such a setup, you have to get **all of the intermediate layers** to return **full sequence** of outputs\n",
    "    * (-) Doesn't perform well compared to baseline since it only consider words, instead of full sequences in inputs\n",
    "    * (-) `SimpleRNN` isn't good at processing long sequences, such as text\n",
    "    * (-) Although, it should theoretically be able to retain at time `t` information about inputs seen many timesteps before, in practice, such **long-term dependencies are <u>impossible</u> to learn**\n",
    "        * (-) Suffers from a major issue called **vanishing gradient problem**, which is a problem when you keep adding layers to a network where the **network becomes untrainable**\n",
    "        ![seq_rnn](seq_rnn.png)\n",
    "        * Theoretical reasons for the **vanishing gradient effect** was studied by Hochreiter and Schmidhuber in the early 1990s\n",
    "        * Since the long-term information has to sequentially travel through all cells before getting to the present processing cell, it can be **easily corrupted** by being **multiplied many times by small numbers < 0**\n",
    "    * In general, `SimpleRNN` is too simplistic to be of real use\n",
    "* **Long Short-Term Memory (`LSTM`)** - developed by Hochreiter and Schmidhuber (1997) created `LSTM` layer that **saves information for later**, thus preventing older signals from gradually vanishing during processing\n",
    "    ![lstm](lstm.png)\n",
    "    * Both `LSTM` and `GRU` were designed to solve the **vanishing gradient effect**\n",
    "    * `LSTM` can be seen as **multiple switch gates**, and a bit like **ResNet** where it can **bypass units and thus remember longer time steps**\n",
    "    ![seq_lstm](seq_lstm.png)\n",
    "    * The `LSTM` layer is a variant on the `SimpleRNN` layer (which adds a way to carry information across many timesteps)\n",
    "    * Imagine a conveyor belt running parallel to the sequence you're processing. **Information from the sequence** can **jump onto the conveyor belt at any point**, be **transported to a later timestep**, and **jump off intact when you need it**\n",
    "    ![simple_to_lstm](simple_to_lstm.png)\n",
    "    * The diagram above adds an **additional data flow** that **carries** information **across timesteps**\n",
    "        * Call its values at different timesteps `Ct`, where `C` stands for **carry**\n",
    "        * This information will have the following impact on the cell:\n",
    "            * It will be **combined with the input connection and the recurrent connection** (via a dense transformation: a dot product with a weight matrix followed by a bias add and the application of an activation function)\n",
    "            * It will **affect the state being sent to the next timestep** (via an activation function and a multiplication operation)\n",
    "        * Conceptually, the **carry dataflow** is a way to **modulate the next output and the next state**\n",
    "        * The way the next value of the carry dataflow is computed uses **three distinct transformations with their own weight matrices** \n",
    "    * Pseudocode details of LSTM architecture:\n",
    "        ```python\n",
    "        output_t = activation(dot(state_t, U_o) + dot(input_t, W_o) + dot(C_t, V_o + b_o)\n",
    "        i_t = activation(dot(state_t, U_i) + dot(input_t, W_i) + b_i)\n",
    "        f_t = activation(dot(state_t, U_f) + dot(input_t, W_f) + b_f)\n",
    "        k_t = activation(dot(state_t, U_k) + dot(input_t, W_k) + b_k)\n",
    "        c_t + 1 = i_t * k_t * c_t * f_t\n",
    "        ```\n",
    "        * Obtain the **new carry state** (the next `c_t`) by **combining `i_t, f_t, and k_t`**\n",
    "    * Interpreting each of the operations of `LSTM`:\n",
    "    ![anatomy_lstm](anatomy_lstm.png)\n",
    "        * Multiplying `c_t` and `f_t` is a way to **deliberately forget irrelevant information** in the carry dataflow\n",
    "        * `i_t` and `f_t` provide **information about the present**, updating the carry track with **new information**\n",
    "        * These interpretations don't mean much since what the weights **actually** do is determined by the **contents of the weights parameterizing** them \n",
    "    * The weights are **learned in an end-to-end fashion**, starting over with each training round, making it **impossible to credit this or that operation with a specific purpose**\n",
    "        * The specification of an RNN cell determines your **hypothesis space** (the space in which you'll search for a good model configuration during training)\n",
    "        * It **doesn't** determine what the **cell does**; that is done by the **cell weights**\n",
    "        * The same cell with different weights can be doing very different things\n",
    "        * The combination of operations making up an RNN cell is better interpreted as a **set of constraints** on your search, **<u>not</u> as a design** in an engineering sense\n",
    "        * The **choice of such constraints** (how to implement RNN cells) is better left to **optimization algorithms** (e.g. **genetic algorithms** or **reinforcement learning processes**) tahn to human engineers\n",
    "    * In summary, `LSTM` cell allows past information to be reinjected at a later time, thus fighting the vanishing-gradient problem\n",
    "* **Concrete `LSTM` example in Keras**\n",
    "    * Only specify the output dimensionality of the `LSTM` layer\n",
    "        * Leave every other argument (there are many) at the Keras defaults\n",
    "        * Keras has good defaults, and the things will almost always \"just work\" without you having to spend time tuning parameters by hand\n",
    "    * `LSTM` will perform better if you tune hyperparameters such as **embeddings dimensionality** or the `LSTM` output dimensionality\n",
    "    * Could also add some regularization\n",
    "    * `LSTM` is good at analyzing **global, long-term structure**, which isn't great for **sentiment analysis** problem\n",
    "    * There are far more difficult NLP problems that the strength of `LSTM` is more apparent: \n",
    "        * **Question-answering**\n",
    "        * **Machine translation**\n",
    "        * Natural language text compression\n",
    "        * Handwriting recognition\n",
    "        * Speech recognition\n",
    "* Issues with `LSTM`/`GRU` technique:\n",
    "    * (-) Still has sequential path from older past cells to the current one with an even more complicated path using **additive** and **forget** branches\n",
    "    * (-) Remembers sequences of 100s, but **not** 1000s or 10,000s or more\n",
    "    * (-) `LSTM`/`GRU` RNNs are not hardware friendly\n",
    "        * (-) `RNN`/`LSTM` are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions\n",
    "        * `LSTM` requires 4 linear layer (MLP layer) per cell to run at and for each sequence timestep\n",
    "        * Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Advanced use of **recurrent neural networks**\n",
    "* **Three** advanced techniques for **improving the performance and generalization power** of recurrent neural networks (demonstrated using **temperature-forecasting** problem):\n",
    "    1. **Recurrent dropout** - specific, built-in way to use **dropout** to fight overfitting in recurrent layers\n",
    "    2. **Stacking recurrent layers** - **increases the representational power** of the network (at the cost of higher computational loads)\n",
    "    3. **Bidirectional recurrent layers** - presents the same information to a recurrent network in **different** ways, **increasing accuracy** and **mitigating forgetting issues**\n",
    "* Timeseries problem pre-processing:\n",
    "    * View data from different periodicity to see what the trend looks like\n",
    "    * No vectorization needed since data is already numerical\n",
    "    * However, each timeseries data is on a **different scale** (e.g. temperature between -20 to 30 and atmospheric pressure is around 1,000)\n",
    "        * Normalize each timeseries independently so that they all take small values on similar scale\n",
    "        * Preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation (for the training data)\n",
    "* Try to establish a **baseline, non-ML score**:\n",
    "    * Use a simple, common-sense approach to serve as a **sanity check** that will establish a baseline that we'll beat in order to **demonstrate the usefulness of a more-advanced machine-learning models**\n",
    "    * Such a common-sense baselines can be useful when you're **approaching a new problem for which there is no known solution yet**\n",
    "    * For example, for a **unbalanced classification tasks**, where some classes are much more common than others\n",
    "        * If your dataset contains 90% instances of class A and 10% instances of class B, then a common-sense approach to the classification task is to **always predict \"A\" when presented with a new sample**\n",
    "        * Such a classifier is 90% accurate overall, and any learning-based approach should therefore **beat this 90% score** in order to demonstrate usefulness\n",
    "        * Sometimes such elementary baselines can prove surprisingly hard to beat\n",
    "    * In this case, the temperature timeseries can safely be assumed to be **continuous** (the temperatures tomorrow are likely to be close to the temperatures today) as well as **periodical with a daily period**\n",
    "        * Thus a common-sense approach is to **always predict that the temperature 24 hours** from now will be **equal to the temperature right now**\n",
    "    * Evaluate this approach using **mean absolute error (MAE)** metric: `np.mean(np.abs(preds - targets))`\n",
    "* Also try to establish a **basic ML approach**:\n",
    "    * Use a simple, cheap ML model (e.g. small, densely connected networks) before looking into a complicated and computationally expensive models such as RNNs\n",
    "    * This is the best way to make sure **any further complexity thrown** at the problem is **legitimate** and **delivers real benefits**\n",
    "    * It is possible that the validation losses for this simple ML model are close to the no-learning baseline making the no-learning baseline difficult to outperform\n",
    "* Now try **Gated Recurrent Unit (`GRU`)**\n",
    "    * `GRU` layers work using the same principle as `LSTM`, but they're **somewhat streamlined** and thus **cheaper to run**, although they may **<u>not</u> have as much representational power** as `LSTM`\n",
    "    * The **trade-off** between **computational expensiveness** and **representational power** is seen everywhere in ML\n",
    "* **Using <u>recurrent dropout</u> to fight overfitting** - using the same dropout mask at every timestep\n",
    "    * **Dropout** - randomly zeros out input units of a layer in order to break coincidental correlations in the training data that the layer is exposed to\n",
    "    * It has long been know that **applying dropout before a recurrent layer hinders learning** rather than helping with regularization\n",
    "    * In 2015, Yarin Gal (PhD on Bayesian deep learning) determined the proper way to use dropout with a recurrent network:\n",
    "        * The same **dropout mask** (the same pattern of dropped units) should be **applied at every timestep** instead of a dropout mask that varies **randomly** from timestep to timestep\n",
    "        * In order to regularize the representations formed by the recurrent gates of layers (e.g. `GRU` and `LSTM`), a **temporally constant dropout mask** should be applied to the **<u>inner recurrent activations</u> of the layer** (**a <u>recurrent</u> dropout mask**)\n",
    "        * Using the **same dropout mask at every timestep** allows the network to properly **propagate its learning error through time**\n",
    "            * If you used a **temporally random dropout mask**, it would **disrupt** the error signal and be harmful to the learning process\n",
    "        * Yarin Gal built this mechanism directly into Keras on every recurrent layer where there will be **two dropout-related arguments**: `dropout` and `recurrent_dropout`\n",
    "            * **`dropout`** - a float specifying the dropout rate for input units of the layer\n",
    "            * **`recurrent_dropout`** - specifying the dropout rate of the recurrent units\n",
    "        * Networks being **regularized with dropout** always take **longer to fully converge**, it is best to train the network for **twice as many epochs**\n",
    "* **Stacking recurrent layers** - increasing the number of units in the layers or adding more layers to recurrent neural network\n",
    "    * If the neural network model is no longer overfitting, but seem to have hit a performance bottleneck, consider **increasing the capacity of the network**\n",
    "        * It is generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (assuming you've already mitigated overfitting using dropout)\n",
    "    * Increasing network capacity is typically done by increasing the number of units in the layers or adding more layers\n",
    "        * e.g. For **Google Translate** algorithm, it uses seven large `LSTM` layers\n",
    "    * To stack recurrent layers on top of each other in Keras, all intermediate layers should **return their full sequence of outputs (a 3D tensor)** rather than their output at the last timestep (`return_sequences=True`)\n",
    "* **Using Bidirectional RNNs** - exploits the order sensitivity of RNNs by **processing a sequence both ways** that will **catch patterns that may be overlooked** by a unidirectional RNN\n",
    "    ![bidirectional_rnn](bidirectional_rnn.png)\n",
    "    * Can offer greater performance than a regular RNN on certain tasks\n",
    "    * Frequently used in NLP (often called the \"Swiss Army knife\" of deep learning of NLP)\n",
    "    * RNNs are notably order dependent, or time dependent (shuffling or reversing the timesteps can completely change the representations the RNN extracts from the sequence)\n",
    "    * It consists of using **two regular RNNs** (e.g. `GRU` and `LSTM` layers), each of which **processes the input sequence in the opposite direction** (**chronologically** and **antichronologically**), and then **merging their representations**\n",
    "    * By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN\n",
    "    * The underlying `GRU` layer will typically be better at remembering the recent past than the distant past, and naturally the more recent events are more predictive than older data points for the problem (e.g. for timeseries forecast problems like weather)\n",
    "        * This isn't true for many other problem like natural language since the **importance of a word in understanding a sentence** <u>isn't</u> usually dependent on **its position in the sentence**\n",
    "    * On text dataset, reversed-order processing works just as well as chronological process confirming the hypothesis that word order **does** matter in understanding language, but **which** order you use isn't crucial\n",
    "        * More importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences\n",
    "        * In ML, representations that are **different yet useful** are always worth exploiting, and the more they differ, the better\n",
    "        * It offers new angles to look at your data, capturing aspects of the data that were missed by other approaches (boosting performance on a task) -- **ensembling**\n",
    "* Other ways to **improve performance**:\n",
    "    * Adjust the number of units in each recurrent layer in the stacked setup\n",
    "    * Adjust the learning rate used by the `RMSprop` optimizer\n",
    "    * Try using `LSTM` insteadof `GRU` layers\n",
    "    * Try using a bigger densely connecte regressor on top of the recurrent layers (e.g. bigger `Dense` layer or even a stack of `Dense` layers)\n",
    "    * Run best-performing models on the test set to check that you aren't overfitting to the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) **Sequence processing** with **convnets**\n",
    "* The same properties that make convnets excel at computer vision also make them highly relevant to **sequence processing**\n",
    "* **Time** can be treated as a **spatial dimension**, like the height or width of a 2D image\n",
    "* Such **1D convnets** can be **competitive** with RNNs on certain sequence-processing problems\n",
    "    * (+) Usually considerably cheaper computational cost\n",
    "    * Recently, 1D convnets, typically used with **dilated kernels**, have been used with great success for **audio generation** and **machine translation**\n",
    "    * It has been long known that **small 1D convnets** can offer a **fast alternative to RNNs** for simple tasks (e.g. **text classification** and **timeseries forecasting**)\n",
    "* Understanding **1D convolution** for sequence data:\n",
    "    * The convolution layers introduced previously were 2D convolutions, extracting 2D patches from image tensors and applying an identical transformation to every patch (for image convnets)\n",
    "    ![1d_conv](1d_conv.png)\n",
    "    * In the same way, you can use **1D convolutions**, extracting **local 1D patches** (subsequences) from sequences\n",
    "    * Such 1D convolution layers can **recognize local patterns in a sequence**\n",
    "    * Because the **same input transformation** is performed on every patch, a pattern learned at a **certain position** in a sentence can later be recognized at a **different position** (making 1D convnets **translation invariant for temporal translations**)\n",
    "        * e.g. A 1D convnet processing **sequences of characters** using **convolution windows of size 5** should be able to **learn words or word fragments of length 5 or less**, and it should be able to **recognize these words in any context** in an input sequence\n",
    "        * A character-level 1D convnet is thus able to learn about **word morphology**\n",
    "* **1D pooling** for sequence data:\n",
    "    * Similar to 2D average pooling and max pooling used in convnets to spatially downsample image tensors\n",
    "    * Extracting **1D patches (subsequences)** from an input and outputting the **maximum value** (max pooling) or **average value** (average pooling)\n",
    "    * Just as with 2D convnets, this is used for reducing the length of 1D inputs (**subsampling**)\n",
    "* Implementing a 1D convnet in Keras:\n",
    "    * Takes input 3D tensors with shape `(samples, time, features)` and returns similarly shaped 3D tensors\n",
    "    * 1D convnets are structured in the same way as their 2D counterparts, which consits of a stack of `Conv1D` and `MaxPooling1D` layers, ending in either a global pooling layer or a `Flatten layer`\n",
    "    * One difference is the fact that you can **afford to use a larger convolution windows** with 1D convnets\n",
    "        * With a 2D convolution layer, a 3x3 convolution window contains 3x3 = 9 feature vectors; but with 1D convolution layer, a convolution window of size 3 contains **only** 3 feature vectors (thus can **easily afford 1D convolution windows of size 7 or 9**)\n",
    "    * From example, the validation accuracy is somewhat **less** than that of the `LSTM`, but the **runtime is faster on both CPU and GPU**\n",
    "        * This is a convincing demonstration that a 1D convnet can offer a fast, cheap alternative to a recurrent network on a word-level sentiment-classification task\n",
    "* **Combining CNNs and RNNs** to process **long** sequences:\n",
    "    * Because 1D convnets **process input patches independently**, they **aren't** sensitive to the **order of the timesteps** (beyond a local scale, the size of the convolution windows) unlike RNNs\n",
    "    * To recognize longer-term patterns, you can try stacking many conv layers and pooling layers, but that's still a **fairly weak** way to **induce order sensitivity**\n",
    "        * One way to evidence this weakness is to try 1D convnets on the temperature-forecasting problem, where order-sensitivity is key to producing good predictions\n",
    "        * (-) The convnet looks for patterns anywhere in the input timeseries and has **no knowledge of the temporal position** of a pattern it sees (e.g. toward beginning, toward end, etc.)\n",
    "        * Because more recent data points should be interpreted differently from older data points (for forecasting problem)\n",
    "    * One strategy to combine the **speed and lightness of convnets** with the **order-sensitivity of RNNs** is to use a 1D convnet as a preprocessing step before an RNN\n",
    "    ![cnn_rnn](cnn_rnn.png)\n",
    "        * This is especially **beneficial** when you're dealing with **sequences that are so long** they **can't** realistically be processed with RNNs (e.g. sequences with thousands of steps)\n",
    "        * The convnet will turn the long input sequence into **much shorter (downsampled) sequences** of **higher-level** features\n",
    "        * This sequence of **extracted features** then becomes the **input to the RNN part of the network**\n",
    "    * This technique isn't seen often in research papers and practical applications (possibly because it isn't well known). It is **effective** and ought to be **more common**\n",
    "    * Because this strategy allows you to **manipulate much longer sequences**, you can either: \n",
    "        * Look at the data from longer ago by **increasing the `lookback` parameter** of the data generator\n",
    "        * Look at **high-resolution timeseries** by **decreasing the `step` parameter** of the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **Recurrent Attention** - a better way to look into the past is to use attention modules to summarize all past encoded vectors\n",
    "* If sequential processing is to be avoided, then we can find units that \"look-ahead\" or better \"look-back\", since most of the time we deal with **real-time causal data** where we know the past and want to affect future decisions\n",
    "* These are techniques especially useful for NLP, but aren't particularly applicable for numerical forecasting problem\n",
    "* **Hierarchical Neural Attention Encoder (Attention)** (2015-16)\n",
    "    * **Memories** - basic building block that saves multiple values in a table\n",
    "        ![memories](memories.png)\n",
    "        * Basically just a multi-dimensional array that be of any size and any dimensions\n",
    "        * It can also be composed of multiple banks or heads\n",
    "        * Generally, memory have a **write/read function** that writes to all locations and reads from all location\n",
    "        * An **attention-like module** can focus reading and writing to specific locations \n",
    "    * **Attention Module** - gating functions for memories. If you want to specify which values in an array should be passed through attention, use a linear layer to gate each input by some weighting function\n",
    "    ![attention_mod](attention_mod.png)\n",
    "        * Attention modules can be **soft** when the **weights are real-valued** and the inputs are thus multiplied by values\n",
    "        * Attention is **hard when weights are binary**, and inputs are either 0 or passing through\n",
    "        * Outputs are called **attention head outputs**\n",
    "    * **Hierarchical Neural Attention Encoder** uses **attention modules** to summarize **all past encoded vectors** into a context vector `Ct`\n",
    "    ![attention_enc](attention_enc.png)\n",
    "        * There is a hierarchy of attention modules (similar to Temporal convolutional network (TCN))\n",
    "        * Multiple layers of attention can look at **small portion of recent past** (e.g. 100 vectors) while **layers above can look at 100** of the same attention modules resulting in integrated information of **100x100 vectors**\n",
    "            * This **extends the ability** of hierarchical neural attention encoder to **10,000 past vectors**\n",
    "            * This is the way to look back more into the past and be able to influence the future\n",
    "    * Looking at the **length of the path** needed to **propagate a representation vector** to the output of the network:\n",
    "        * In hierarchical networks, it is **proportional to `log(N)`** where `N` is the **number of hierarchy layers**\n",
    "        * In contrast, the `T` steps that a RNN needs to do where `T` is the maximum length of the sequence to be remembered, and `T >> N`\n",
    "        * It is easier to remember sequences if you hop 3-4 times, as opposed to hopping 100 times\n",
    "        * This architecture is similary to **neural Turing machine**, but lets the neural network **decide what is read out from memory via attention**. This means the NN will decide which vectors from the past are important for future decisions\n",
    "    * The recurrent attention architecture **stores all previous representations in memory** (unlike neural Turing machines)\n",
    "        * (-) This is be rather inefficient since it is like storign the representation of every frame in a video (where most representation vector do not change frame-to-frame, so we are really storing too much of the same information)\n",
    "        * Can try to add another unit to prevent correlated data to be stored\n",
    "        * e.g. A unit that **prevents** storage of vectors **too similar** to previously stored ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8) **Sequence Masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
